{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd04effb-be72-41de-825f-f834935f6dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anwer/.local/lib/python3.12/site-packages/anndata/utils.py:429: FutureWarning: Importing read_csv from `anndata` is deprecated. Import anndata.io.read_csv instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/anwer/.local/lib/python3.12/site-packages/anndata/utils.py:429: FutureWarning: Importing read_excel from `anndata` is deprecated. Import anndata.io.read_excel instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/anwer/.local/lib/python3.12/site-packages/anndata/utils.py:429: FutureWarning: Importing read_hdf from `anndata` is deprecated. Import anndata.io.read_hdf instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/anwer/.local/lib/python3.12/site-packages/anndata/utils.py:429: FutureWarning: Importing read_loom from `anndata` is deprecated. Import anndata.io.read_loom instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/anwer/.local/lib/python3.12/site-packages/anndata/utils.py:429: FutureWarning: Importing read_mtx from `anndata` is deprecated. Import anndata.io.read_mtx instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/anwer/.local/lib/python3.12/site-packages/anndata/utils.py:429: FutureWarning: Importing read_text from `anndata` is deprecated. Import anndata.io.read_text instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/anwer/.local/lib/python3.12/site-packages/anndata/utils.py:429: FutureWarning: Importing read_umi_tools from `anndata` is deprecated. Import anndata.io.read_umi_tools instead.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9e37691-3d8b-40aa-af11-c47c2dbb675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HC_path = '../multi_region_single_cell/HC//minor_cell_type/'\n",
    "adata = sc.read_h5ad(f'{HC_path}Ast.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1dbc225-247d-47c6-8354-56518c16519c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Per.h5ad',\n",
       " 'Opc.h5ad',\n",
       " 'Oli.h5ad',\n",
       " 'Ast.h5ad',\n",
       " 'Mic.h5ad',\n",
       " 'SMC.h5ad',\n",
       " 'End.h5ad',\n",
       " 'Inh.h5ad',\n",
       " 'Fib.h5ad',\n",
       " 'CAM.h5ad',\n",
       " 'CPEC.h5ad',\n",
       " 'T.h5ad',\n",
       " 'Exc.h5ad',\n",
       " 'Epd.h5ad']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_types = []\n",
    "for i in os.listdir(HC_path):\n",
    "    if '.h5ad' in i:\n",
    "        cell_types.append(i)\n",
    "labels  = ['clinical_AD_Label','Pathological_AD_Label'] \n",
    "cell_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60452659-08c7-487c-8d54-47fd51b90290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7d9eb8a-00c1-4f1a-834a-0643bde3f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genes = adata.var.varnames.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bab33853-9467-49f8-8fa2-70050c9e9438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per.h5ad\n",
      "clinical_AD_Label\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 342.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 185\u001b[0m\n\u001b[1;32m    183\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m    184\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m--> 185\u001b[0m best_test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m accuracy_dct[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m best_test_accuracy\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_dct)\n",
      "Cell \u001b[0;32mIn[6], line 101\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m    100\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 101\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    104\u001b[0m test_accuracy  \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/adam.py:159\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    156\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     adam(\n\u001b[1;32m    169\u001b[0m         params_with_grad,\n\u001b[1;32m    170\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/adam.py:115\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    113\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 342.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def return_ensemble_id(x,df_ensemble):\n",
    "    try:\n",
    "        return df_ensemble[df_ensemble['1']==x]['0'].values[0]\n",
    "    except: \n",
    "        return np.nan\n",
    "\n",
    "def return_clinical_label(x):\n",
    "    if x['clinical_diagnosis'] == 'AD':\n",
    "        return 1.0\n",
    "    elif x['clinical_diagnosis'] == 'NCI':\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 2.0\n",
    "\n",
    "\n",
    "def return_pthological_label(x):\n",
    "    if x['pathological_diagnosis'] == 'AD':\n",
    "        return 1.0\n",
    "    elif x['pathological_diagnosis'] == 'No AD':\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 2.0\n",
    "\n",
    "def return_clinical_pthological_label(x):\n",
    "    if x['clinical_pathological_AD'] == 'AD_with_Plaques':\n",
    "        return 1.0\n",
    "    elif x['clinical_pathological_AD'] == 'NCI_with_No_Plaques':\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 2.0\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "\n",
    "clinical_data = pd.read_csv('../clinical_single_cell.csv')\n",
    "\n",
    "df_ensemble = pd.read_csv('../df_ensemble.csv')\n",
    "\n",
    "\n",
    "\n",
    "gene_excel = pd.ExcelFile('../202141022_Genelist_m6aproject_BED.xlsx')\n",
    "m6a = pd.read_excel(gene_excel, 'm6A')\n",
    "m6a = m6a.iloc[2:,]\n",
    "m6a_genes = m6a['m6A related genes/proteins'].tolist()\n",
    "\n",
    "mito = pd.read_excel(gene_excel, 'Mitochondria')\n",
    "mito  = mito.iloc[2:,]\n",
    "mito_genes = mito['Mitochondria genes'].tolist()\n",
    "\n",
    "\n",
    "mitophagy = pd.read_excel(gene_excel, 'Mitophagy')\n",
    "mitophagy = mitophagy.iloc[2:,]\n",
    "mitophagy_genes = mitophagy['Autophagy_Mitophagy_lysosome'].tolist()\n",
    "\n",
    "AD = pd.read_excel(gene_excel, 'AD')\n",
    "AD   = AD.iloc[2:,]\n",
    "AD_genes = AD[\"Alzheimer's disease - risk factors\"].tolist()\n",
    "\n",
    "\n",
    "mito = pd.read_excel(gene_excel, 'Mitochondria')\n",
    "mito  = mito.iloc[2:,]\n",
    "mito_genes = mito['Mitochondria genes'].tolist()\n",
    "mito_genes[0:5]\n",
    "\n",
    "lysosome = pd.read_excel(gene_excel, 'Lysosome')\n",
    "lysosome  = lysosome.iloc[2:,]\n",
    "lysosome_genes = lysosome['Lysosomes'].tolist()\n",
    "\n",
    "genes_set  =  [all_genes,m6a_genes,mito_genes,mitophagy_genes,AD_genes,mito_genes,lysosome_genes]\n",
    "genes_set_name  =  ['all_genes','m6a_genes','mito_genes','mitophagy_genes','AD_genes','mito_genes','lysosome_genes']\n",
    "\n",
    "\n",
    "        \n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "            outputs = model(inputs).squeeze(1)\n",
    "            preds = torch.sigmoid(outputs) > 0.5  # Convert logits to binary predictions\n",
    "            correct += (preds == labels.squeeze(1)).sum().item()\n",
    "            total += labels.size(0)\n",
    "            Accuracy = 100 * correct / total\n",
    "    return Accuracy\n",
    "\n",
    "def train_model(model, train_loader,test_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze(1)  # Remove extra dimension\n",
    "            loss = criterion(outputs, labels.squeeze(1).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        test_accuracy  = evaluate_model(model, test_loader)\n",
    "        train_accuracy = evaluate_model(model, train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}, TrainAccuracy: {train_accuracy:.2f}, Test_Accuracy: {test_accuracy:.2f}\")\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "    \n",
    "    return best_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "accuracy_dct = dict()\n",
    "\n",
    "for i in cell_types:\n",
    "    print(i)\n",
    "    adata = sc.read_h5ad(f'{HC_path}{i}')\n",
    "    temp_data = pd.merge(adata.obs.reset_index(), clinical_data, on =  'projid', how='inner').drop_duplicates()\n",
    "    temp_data = temp_data.set_index(adata.obs.index)\n",
    "    adata.obs = temp_data\n",
    "    adata.obs['clinical_AD_Label'] = adata.obs.apply(return_clinical_label,axis=1)\n",
    "    adata.obs['Pathological_AD_Label'] = adata.obs.apply(return_pthological_label,axis=1)\n",
    "    adata.obs['Clinical_Pathological_AD_Label'] = adata.obs.apply(return_clinical_pthological_label,axis=1)\n",
    "    adata.obs.drop('obsnames', axis=1, inplace=True)\n",
    "    \n",
    "    for  j in labels:\n",
    "        print(j)\n",
    "        subset_adata = adata[adata.obs[j].isin([0.0,1.0]),:]\n",
    "        Neg = subset_adata.obs[subset_adata.obs[j] ==0.0]\n",
    "        Pos = subset_adata.obs[subset_adata.obs[j] ==1.0]\n",
    "        max_sample = min(len(Neg),len(Pos))\n",
    "        Neg = subset_adata.obs[subset_adata.obs[j] ==0.0].iloc[0:max_sample,:]\n",
    "        Pos = subset_adata.obs[subset_adata.obs[j] ==1.0].iloc[0:max_sample,:]\n",
    "        data = pd.concat([Neg,Pos])\n",
    "        subset_adata = subset_adata[data.index,:]\n",
    "        count_matrix_df = pd.DataFrame(\n",
    "                    subset_adata.X.toarray(),  # Convert sparse matrix to dense array if needed\n",
    "                    index=subset_adata.obs.index,  # Cell IDs as row indices\n",
    "                    columns=subset_adata.var.index  # Gene names as column indices\n",
    "                )\n",
    "    \n",
    "        \n",
    "        for l,k in enumerate(genes_set):\n",
    "            #print(k)\n",
    "            keep_genes = list(set(k).intersection(set(count_matrix_df.columns)))\n",
    "            count_matrix_df = count_matrix_df[keep_genes]\n",
    "            label_df = pd.DataFrame(subset_adata.obs[j])\n",
    "            name = genes_set_name[l]\n",
    "\n",
    "\n",
    "             # ML part\n",
    "            X = torch.tensor(count_matrix_df.values, dtype=torch.float32)\n",
    "            y = torch.tensor(label_df.values, dtype=torch.long)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "            X_train, X_test, y_train, y_test = X_train.to(device) , X_test.to(device)  , y_train.to(device) , y_test.to(device)\n",
    "\n",
    "            dataset_train = TensorDataset(X_train, y_train)\n",
    "            dataset_test = TensorDataset(X_test, y_test)\n",
    "            input_size = X.shape[1]\n",
    "            hidden_size =      X.shape[1]//2\n",
    "            output_size = 1  # Binary classification\n",
    "            model = NeuralNet(input_size, hidden_size, output_size).to(device) \n",
    "            train_loader = DataLoader(dataset_train, batch_size=4096, shuffle=True)\n",
    "            test_loader = DataLoader(dataset_test, batch_size=4096, shuffle=False)\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "            best_test_accuracy = train_model(model, train_loader,test_loader, criterion, optimizer, epochs=1000)\n",
    "\n",
    "            accuracy_dct[f'{i.split('.')[0]}.{name}.{j}'] = best_test_accuracy\n",
    "            print(accuracy_dct)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d6241-d8eb-43a6-95eb-7419b1a20c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "1000//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ca258-f83a-4633-952a-abc880a5db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i in x:\n",
    "    if i == True:\n",
    "        li.append(1)\n",
    "    else:\n",
    "        li.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee3685-3afd-4b67-be40-68a21e005b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "for i,j in enumerate(li):\n",
    "    if y[i] == j:\n",
    "        s +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ef39d-2e8f-4516-88b2-2890d3ca2d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "s/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab2e7e-e17d-46a3-9493-3144580e9df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e8789-ef83-4909-a953-28edb6bf8ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
